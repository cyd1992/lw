# 1 绪论

## 1.1课题背景
-------------

本课题在基于国家数据广播中心原有的系统的基础上，进一步实现了支持多人收看电视直播与回看节目的一整套解决方案。该系统可以将卫星采集下来的高清电视节目通过流媒体服务器，变为可供观看的视频流。同时将直播节目录制成视频文件以供回看。最后，跨平台的视频播放器保证用户可以随时随地观看到视频节目。


## 1.2 选题的意义与研究价值
------------------------

的说法是的范德萨发斯蒂芬

## 1.3 作者的主要工作
------------------

作者完成了整套系统的设计与实现。包括整体软件的架构设计、回看服务器软件设计与开发，数据库设计，节目管理系统的设计与开发以及各个平台播放器客户端的设计与开发。

## 1.4 论文的组织结构
-------------------




#2 整体方案设计



## 2.1 系统方案设计
----------------
系统由前端、边缘服务器及用户构成。blablabla 
整体系统框图如图所示

![pic1](src/3.png =x400) 


###### 前端功能
1. 卫星直转系统接收来自卫星的电视节目及自办节目，重新编码为低码率H.264直播流，分发到直播服务器。
2. 直播服务器为流媒体服务器，
3. 提供DNS域名解析服务，采集边缘服务器状态，指导用户连接到合适的边缘服务器
4. 提供NTP服务，为整个网络提供基于GPS的时间基准
5. 提供热备份。

##### 边缘服务器功能
1. 边缘服务器具备双网口，跨网段使用；
2. 主干网口连接主干网，接收直播流、同步点播节目及接收EPG；
3. 子网段口连接子网段，提供不高于3000路并发的直播、回看、点播服务；
4. 主干网口也可提供不高于300路并发的直播、回看、点播服务。

##### 用户功能及种类
1. 用户位于子网段内，通过网页或APP访问TV.com，观看直播、回看、点播节目；
2. 用户终端支持电脑、平板及苹果手机的网页访问，安卓盒子及手机的APP访问；
3. 子网段内可挂接多个WIFI无线路由器，提供无线访问能力。


------------------------

网络拓扑图如图所示

![pic3](src/2.png =x350)

主（备）前端连主干网，提供直播流、点播节目、DNS（域名解析）、EPG（电子节目单）及NTP（网络时间协议）服务。外部4G信号接入前端，提供EPG及远程维护功能。
边缘服务器具备双网口，一端连主干网，接收直播流；另一端连子网段，提供直播、回看、点播服务。当子网段用户过多时，可安装多套边缘服务器。
     
用户分布于各个子网段内，通过前端DNS解析，得到最近的边缘服务器IP，进而申请直播、回看、点播服务。当访问主前端失败时，可自动访问备份前端。

前端由卫星直转系统，电视转码系统，高清编码系统和播出服务器系统构成。

------------------------



## 2.2 前端系统
前端系统完成IPTV及数字电视播出。为可靠起见，前端系统采用双备份，在主前端故障时，IPTV会自动切换到备份前端，数字电视系统需要手动切换。如图所示

![pic2](src/4.png =x400) 



前端完成IPTV及数字电视播出，输入为卫星电视与4G信号，输出为有线电视RF信号及IPTV万兆光接口。
前端网络分为内外两张网，内网完成核心的电视转码系统及高清编码系统；外网提供主直播/点播服务和主干网其它服务。内网通过路由器与外网连接，以保证内网的安全可靠性。
内网通过管理服务器接入4G信号，实现外部远程系统管理和EPG获取。
前端分为主前端与备份前端，两者结构相同，相应IP地址不同，方便用户及边缘服务器自动切换。







##### 卫星直转系统
卫星直转系统介绍见“岛礁有线电视方案”，主要完成100套卫星节目接收，解调，拼接，调制，最终输出为有线电视RF信号。
IPTV系统节目源大部分来自卫星直转系统，通过系统改造，将节目通过增加的ASI接口输出至“数字电视转IP”单元。
     
##### 电视转码系统
电视转码系统由数字电视IP转换及转码机两部分构成，数字电视IP转换接收ASI输入的多套MCPC电视节目，转为SCPC的单节目UDP流，发送到指定的转码机；转码机完成多路UDP节目接收、视音频解码，H.264视频/AAC音频编码，通过RTMP协议发送到直播服务器。
单台数字电视转IP完成6路ASI接收，本系统有12路ASI流，需要2台数字电视IP转换设备；单台转码机完成16套标清或8套高清节目转码，需5台转码机完成80套节目实时转码。

##### 高清编码系统
高清编码系统由32台高清机顶盒和或HDMI高清编播系统（自办高清直播节目），2主2从高清编码机，蓝网、广播接收机构成。完成32套高清统计复用编码，同时插入蓝网、广播节目，输出的ASI送到直转系统调制器，输出的IPTV通过RTMP协议发送到直播服务器。

##### 播出服务器
播出服务器由直播服务器、点播服务器、DNS/EPG/NTP服务器构成。
直播服务器完成100套IPTV节目RTMP接收及向多个边缘直播服务器分发功能；
点播服务器完成点播节目及蓝色海疆数据广播节目接收、存储及与边缘点播服务器同步；  
DNS服务器完成DNS域名解析及边缘服务器信息收集与配置，返回用户合适边缘服务器的IP地址；
EPG服务器通过4G得到外部实时EPG，为边缘的回看服务器自动录制提供电子节目单。EPG是一个实时动态数据库，需要购买及长期人工维护；
NTP服务器通过GPS/北斗校时，为系统提供精确可靠的时间信息；
另外，播出系统还配备管理服务器，通过4G远程配置与监视系统工作。
   

##### 前端网络布局
 前端网络分为内外两张网，内网完成核心的电视转码系统及高清编码系统；外网提供主直播/点播服务和主干网其它服务。内网通过路由器与外网连接，以保证内网的安全性。
内网通过管理服务器接入4G信号，实现外部远程系统管理和EPG获取。
前端分为主前端与备份前端，两者结构相同，功能相同，仅IP地址不同，互为热备份。
 



------------------------------

本课题在已有的卫星只转系统与电视转码系统的基础上，进一步实现岛礁直播点播系统以供岛礁上的用户可以方便的观看直播与点播的电视节目。
系统用例图如图所示

![pic5](src/4.jpg =x400)











# 3  直播与回看服务器的设计与实现
## 3.1 直播与回看管理软件的设计与实现
--------------------------------




系统部署图如图所示


其中


## 3.2 直播服务器的设计与实现
--------------------------
### 3.2.1 开源流媒体服务器srs
SRS（Simple Rtmp Server）是MIT协议的开源流媒体服务器项目。
SRS（Simple Rtmp Server）的定位是运营级的互联网直播服务器集群，追求更好的概念完整性和最简单实现的代码。<br />

* 运营级：<br/>
商业运营追求极高的稳定性，良好的系统对接，以及错误排查和处理机制。譬如日志文件格式，reload，系统HTTP接口，提供init.d脚本，转发，转码，边缘回多源站，都是根据CDN运营经验作为判断这些功能作为核心的依据。
* 互联网：<br/>
互联网最大的特征是变化，唯一不变的就是不断变化的客户要求，唯一不变的是基础结构的概念完整性和简洁性。互联网还意味着参与性，听取用户的需求和变更，持续改进和维护。
* 直播服务器：<br>
直播和点播这两种截然不同的业务类型，导致架构和目标完全不一致，从运营的设备组，应对的挑战都完全不同。两种都支持只能说明没有重心，或者低估了代价。
* 集群：<br>
FMS(AMS)的集群还是很不错的，虽然在运营容错很差。SRS（Simple Rtmp Server）支持完善的直播集群，Vhost分为源站和边缘，容错支持多源站切换、测速、可追溯日志等。
* 概念完整性：
虽然代码甚至结构都在变化，但是结构的概念完整性是一直追求的目标。从SRS（Simple Rtmp Server）服务器，P2P，ARM监控产业，MIPS路由器，服务器监控管理，ARM智能手机，SRS（Simple Rtmp Server）的规模不再是一个服务器而已。
* 简单实现：
对于过于复杂的实现，宁可不加入这个功能，也不牺牲前面提到的要求。对于已经实现的功能的代码，总会在一个版本release前给予充分的时间来找出最简答案。不求最高性能，最优雅，最牛逼，但求最简单易懂。

与nginx-rtmp相比，SRS(Simple Rtmp Server)单进程能支持9000并发，nginx-rtmp单进程最多支持3000个，单进程的性能SRS(Simple Rtmp Server)是nginx-rtmp的三倍。srs针对直播服务器有如下优化：

第一点，st-load，这个是SRS(Simple Rtmp Sever)能做到高性能的最重要的原因，一个st-load可以模拟2000+的客户端。一个牛逼的benchmark的工具；如果没有st-load，如何知道系统的性能瓶颈在哪里？总不能打开3000个flash页面播放rtmp流吧？开启3000个ffmpeg来抓流？不靠谱。这就是高性能第一定律：高性能不是想象和猜测粗来的，而是测试、调试和改进粗来的。
第二点，gperf/gprof性能benchmark功能。在编译SRS(Simple Rtmp Sever)时，就可以打开gcp或者gprof的性能分析选项，灰常方便就可以拿到数据。缩短了改进和优化的开发周期。
第三点，引用计数的msgs避免内存拷贝。从编码器收到的video/audio数据，转换成SrsSharedPtrMessage放到每个连接的发送队列，避免每个都拷贝一次；因为发送给每个客户端的消息(不是chunked包)头可能不一样，譬如时间戳不一样，但是消息的payload是一样的。
第四点，使用writev发送chunked包，避免消息到chunked包的内存拷贝。可以开辟一个header的缓冲区，专门放每个chunked包的header，然后用iovc保存头的指针和大小，payload的指针和大小，用writev就可以一次发送。
第五点，mw(merged-write)技术，即一次发送多个消息。虽然每个消息使用writev可以避免拷贝，还有更高效的是一次发送多个消息，即把多个消息的chunked头写在header的缓冲区，iovc保存多个消息的chunked头和payload指针，一次writev发送多个消息。这个是最关键所在。
第六点，减少timeout recv，每个连接都是一个st-thread在服务。在发送之前，线程得尝试从连接收取消息，譬如客户端的stop之类的；所以只能recv时指定timeout，譬如300毫秒如果还没有收到消息，就发送连接队列中的消息。这个会导致st的timeout红黑树操作频繁。实际上，可以直接开启一个recv线程，因为客户端的消息非常少，避免timeout接收。
第七点，fast buffer和cache。譬如每次取消息的数组，使用cache；使用fast buffer避免频繁删除；使用header的cache。
第八点，vector还是list？有的地方看起来list更高效，譬如simple buffer这种频繁删除头，以及在结尾加入数据，看起来是list应该做的事情。但是实际上测试发现，vector比list高10%性能。所以，回到第一点，高性能不是猜测和想象粗来的；有的时候有些代码写得很慢，但是这个频率非常低，那么就不要考虑性能，而要考虑可读性。我觉得可以算是高性能第二定律：不要总是考虑高性能，可读性更重要。

### 3.2.2 流媒体协议分析
1 rtmp<br>

RTMP(Real Time Messaging Protoc01)协议 是传输层协议,是基于TCP的协议。创建的是长 连接。它像一个用来装数据包的容器,这些数据可以是AMF格式的数据,也可以是FI。V中的视/音 频数据。在RTMP中控制信息和媒体数据都称之 为message。由于PTMP是基于TCP的,并且 message的长度会很长,所以RTMP采用了一种 分片的策略。每一个分片称为chunk。每个mes- sage被分解成一个或多个chunk。


2 hls<br>


### 3.2.2 srs的部署与使用








## 3.3 点播服务器的设计与实现
--------------------------






# 4 客户端播放器的设计与实现

## 4.1 概述
----------

## 4.2 网页播放器
----------------


## 4.3 手机浏览器播放器
--------------------
## 4.4 安卓手机客户端播放器
------------------------

## 4.5 安卓机顶盒播放器
--------------------

# 5 系统测试与分析












